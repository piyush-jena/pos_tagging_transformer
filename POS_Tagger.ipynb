{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "yFlMnaEfW23i",
        "outputId": "b3450ead-78b6-4637-d767-1edb98d992f1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f0a23040f90>"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import ast\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(1337)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "pZ7Nbon0XvQX"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.read_csv('test_data.csv')\n",
        "train_data = pd.read_csv('train_data.csv')\n",
        "valid_data = pd.read_csv('valid_data.csv')\n",
        "word_vectors = pd.read_csv('wv.csv')"
      ],
      "metadata": {
        "id": "gJ2vdYz1nJ7W"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = {}\n",
        "dictionary_size = word_vectors['vectors'].shape[0]\n",
        "\n",
        "for i in range(dictionary_size):\n",
        "    wv = np.fromstring(word_vectors['vectors'][i][1:-1], sep=' ')\n",
        "    dictionary[word_vectors['word'][i]] = torch.from_numpy(wv)"
      ],
      "metadata": {
        "id": "lIGBT2i8qfnd"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "X_test = []\n",
        "y_test = []\n",
        "\n",
        "X_valid = []\n",
        "y_valid = []\n",
        "\n",
        "for i in range(len(train_data)):\n",
        "    tokens = ast.literal_eval(train_data['tokens'][i])\n",
        "    pos_tags = ast.literal_eval(train_data['pos_tags'][i])\n",
        "\n",
        "    for j in range(len(tokens)):\n",
        "        if (tokens[j] not in dictionary):\n",
        "            X_train += [torch.zeros(64)]\n",
        "        else:\n",
        "            X_train += [dictionary[tokens[j]]]\n",
        "        y_train += [torch.tensor(pos_tags[j])]\n",
        "\n",
        "for i in range(len(valid_data)):\n",
        "    tokens = ast.literal_eval(valid_data['tokens'][i])\n",
        "    pos_tags = ast.literal_eval(valid_data['pos_tags'][i])\n",
        "\n",
        "    for j in range(len(tokens)):\n",
        "        if (tokens[j] not in dictionary):\n",
        "            X_valid += [torch.zeros(64)]\n",
        "        else:\n",
        "            X_valid += [dictionary[tokens[j]]]\n",
        "        y_valid += [torch.tensor(pos_tags[j])]\n",
        "\n",
        "for i in range(len(test_data)):\n",
        "    tokens = ast.literal_eval(test_data['tokens'][i])\n",
        "    pos_tags = ast.literal_eval(test_data['pos_tags'][i])\n",
        "\n",
        "    for j in range(len(tokens)):\n",
        "        if (tokens[j] not in dictionary):\n",
        "            X_test += [torch.zeros(64)]\n",
        "        else:\n",
        "            X_test += [dictionary[tokens[j]]]\n",
        "        y_test += [torch.tensor(pos_tags[j])]"
      ],
      "metadata": {
        "id": "CIZQxGAY95NC"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 10\n",
        "batch_size = 32\n",
        "\n",
        "def get_batch(split):\n",
        "    if split == 'train':\n",
        "        ix = torch.randint(len(X_train) - block_size, (batch_size, ))\n",
        "        x = torch.stack([torch.stack(X_train[i:i+block_size]) for i in ix])\n",
        "        y = torch.stack([torch.stack(y_train[i:i+block_size]) for i in ix])\n",
        "    elif split == 'valid':\n",
        "        ix = torch.randint(len(X_valid) - block_size, (batch_size, ))\n",
        "        x = torch.stack([torch.stack(X_valid[i:i+block_size]) for i in ix])\n",
        "        y = torch.stack([torch.stack(y_valid[i:i+block_size]) for i in ix])\n",
        "    else:\n",
        "        ix = torch.randint(len(X_test) - block_size, (batch_size, ))\n",
        "        x = torch.stack([torch.stack(X_test[i:i+block_size]) for i in ix])\n",
        "        y = torch.stack([torch.stack(y_test[i:i+block_size]) for i in ix])\n",
        "    x, y = x.type(torch.FloatTensor), y\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "1tIre8GoIeUK"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = q.size()[-1]\n",
        "    scaled = torch.matmul(q, k.transpose(-1, -2)) / (d_k ** 0.5)\n",
        "    if mask is not None:\n",
        "        scaled += mask\n",
        "    attention = F.softmax(scaled, dim=-1)\n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.qkv_layer = nn.Linear(d_model , 3 * d_model)\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, max_sequence_length, d_model = x.size()\n",
        "        qkv = self.qkv_layer(x)\n",
        "        qkv = qkv.reshape(batch_size, max_sequence_length, self.num_heads, 3 * self.head_dim)\n",
        "        qkv = qkv.permute(0, 2, 1, 3)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "        values, attention = scaled_dot_product(q, k, v, mask)\n",
        "        values = values.reshape(batch_size, max_sequence_length, self.num_heads * self.head_dim)\n",
        "        out = self.linear_layer(values)\n",
        "        return out\n",
        "\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, parameters_shape, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.parameters_shape=parameters_shape\n",
        "        self.eps=eps\n",
        "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
        "        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
        "        mean = inputs.mean(dim=dims, keepdim=True)\n",
        "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
        "        std = (var + self.eps).sqrt()\n",
        "        y = (inputs - mean) / std\n",
        "        out = self.gamma * y  + self.beta\n",
        "        return out\n",
        "\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, hidden)\n",
        "        self.linear2 = nn.Linear(hidden, d_model)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.tanh(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm1(x + self.attention(x, mask=None))\n",
        "        x = self.norm2(x + self.ffn(x))\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers, block_size, d_input, d_output):\n",
        "        super().__init__()\n",
        "        self.position_embedding_table = nn.Embedding(block_size, d_model)\n",
        "        self.layers = nn.Sequential(*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n",
        "        self.linear = nn.Linear(d_model, d_output)\n",
        "\n",
        "    def forward(self, x, target=None):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        pos_embds = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = x + pos_embds\n",
        "\n",
        "        x = self.layers(x)\n",
        "        logits = self.linear(x)\n",
        "\n",
        "        if target is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            target = target.view(B*T)\n",
        "            loss = F.cross_entropy(logits, target)\n",
        "\n",
        "        return logits, loss"
      ],
      "metadata": {
        "id": "Gl7h7IFsbp_I"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 64\n",
        "num_heads = 4\n",
        "drop_prob = 0.0\n",
        "batch_size = 30\n",
        "max_sequence_length = 64\n",
        "ffn_hidden = 256\n",
        "num_layers = 4\n",
        "learning_rate = 1e-3\n",
        "max_iters = 30000\n",
        "eval_interval = 500\n",
        "eval_iters = 10\n",
        "\n",
        "encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, block_size, 64, 47)"
      ],
      "metadata": {
        "id": "2DEQg_cKl5rn"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = encoder.to(device)\n",
        "print(sum(p.numel() for p in encoder.parameters())/1e6, 'M parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMpFwt9Ol9Nn",
        "outputId": "2544f83c-93ff-42ce-8802-5c5e3d6a2736"
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.203631 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    accuracy = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'valid']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "\n",
        "            N, _ = logits.shape\n",
        "            Y = Y.view(N)\n",
        "            total += N\n",
        "            for i in range(N):\n",
        "                probs = F.softmax(logits[i], -1)\n",
        "                idx = torch.multinomial(probs, num_samples=1)\n",
        "                if (idx in [21, 22, 23, 24, 25, 28, 29] and Y[i] in [21, 22, 23, 24, 25, 28, 29]):\n",
        "                    correct += 1\n",
        "                elif (idx in [37, 38, 39, 40, 41, 42] and Y[i] in [37, 38, 39, 40, 41, 42]):\n",
        "                    correct += 1\n",
        "                elif (idx in [16, 17, 18, 30, 31, 32] and Y[i] in [16, 17, 18, 30, 31, 32]):\n",
        "                    correct += 1\n",
        "                elif (idx not in [21, 22, 23, 24, 25, 28, 29, 37, 38, 39, 40, 41, 42, 16, 17, 18, 30, 31, 32] and Y[i] not in [21, 22, 23, 24, 25, 28, 29, 37, 38, 39, 40, 41, 42, 16, 17, 18, 30, 31, 32]):\n",
        "                    correct += 1\n",
        "\n",
        "        out[split] = losses.mean()\n",
        "        accuracy[split] = (float(correct)/total)\n",
        "    model.train()\n",
        "    return out, accuracy"
      ],
      "metadata": {
        "id": "d-rTT-4xyGUE"
      },
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(encoder.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses, accuracy = estimate_loss(encoder)\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f} accuracy: {accuracy['train']:.4f} valid loss {losses['valid']:.4f} accuracy: {accuracy['valid']:.4f}\")\n",
        "\n",
        "    Xb, yb = get_batch('train')\n",
        "    _, loss = encoder(Xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eq1HLg7aq5of",
        "outputId": "37cf661c-69ab-45e6-bb45-d9fbc6465157"
      },
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 0.4422 accuracy: 0.8713 valid loss 0.4606 accuracy: 0.8730\n",
            "step 500: train loss 0.4458 accuracy: 0.8853 valid loss 0.5097 accuracy: 0.8590\n",
            "step 1000: train loss 0.4349 accuracy: 0.8733 valid loss 0.5195 accuracy: 0.8680\n",
            "step 1500: train loss 0.4231 accuracy: 0.8830 valid loss 0.5006 accuracy: 0.8727\n",
            "step 2000: train loss 0.4764 accuracy: 0.8760 valid loss 0.4928 accuracy: 0.8770\n",
            "step 2500: train loss 0.4212 accuracy: 0.8817 valid loss 0.5215 accuracy: 0.8550\n",
            "step 3000: train loss 0.4145 accuracy: 0.8897 valid loss 0.4301 accuracy: 0.8827\n",
            "step 3500: train loss 0.4362 accuracy: 0.8730 valid loss 0.4335 accuracy: 0.8840\n",
            "step 4000: train loss 0.3926 accuracy: 0.8930 valid loss 0.4677 accuracy: 0.8787\n",
            "step 4500: train loss 0.4429 accuracy: 0.8797 valid loss 0.4439 accuracy: 0.8843\n",
            "step 5000: train loss 0.4366 accuracy: 0.8797 valid loss 0.4667 accuracy: 0.8863\n",
            "step 5500: train loss 0.4028 accuracy: 0.8893 valid loss 0.4855 accuracy: 0.8827\n",
            "step 6000: train loss 0.4233 accuracy: 0.8757 valid loss 0.4827 accuracy: 0.8733\n",
            "step 6500: train loss 0.4088 accuracy: 0.8860 valid loss 0.4421 accuracy: 0.8783\n",
            "step 7000: train loss 0.4369 accuracy: 0.8850 valid loss 0.4478 accuracy: 0.8793\n",
            "step 7500: train loss 0.4217 accuracy: 0.8857 valid loss 0.4670 accuracy: 0.8793\n",
            "step 8000: train loss 0.3898 accuracy: 0.8973 valid loss 0.4657 accuracy: 0.8893\n",
            "step 8500: train loss 0.4074 accuracy: 0.8907 valid loss 0.4763 accuracy: 0.8723\n",
            "step 9000: train loss 0.3913 accuracy: 0.8840 valid loss 0.4910 accuracy: 0.8657\n",
            "step 9500: train loss 0.4189 accuracy: 0.8797 valid loss 0.4745 accuracy: 0.8850\n",
            "step 10000: train loss 0.4419 accuracy: 0.8717 valid loss 0.4686 accuracy: 0.8643\n",
            "step 10500: train loss 0.3958 accuracy: 0.8857 valid loss 0.4788 accuracy: 0.8763\n",
            "step 11000: train loss 0.3911 accuracy: 0.8843 valid loss 0.4720 accuracy: 0.8803\n",
            "step 11500: train loss 0.3982 accuracy: 0.8947 valid loss 0.5032 accuracy: 0.8830\n",
            "step 12000: train loss 0.4039 accuracy: 0.8887 valid loss 0.4591 accuracy: 0.8923\n",
            "step 12500: train loss 0.4054 accuracy: 0.8927 valid loss 0.4679 accuracy: 0.8727\n",
            "step 13000: train loss 0.4445 accuracy: 0.8790 valid loss 0.4707 accuracy: 0.8850\n",
            "step 13500: train loss 0.3705 accuracy: 0.8943 valid loss 0.4296 accuracy: 0.8840\n",
            "step 14000: train loss 0.3763 accuracy: 0.8833 valid loss 0.4631 accuracy: 0.8710\n",
            "step 14500: train loss 0.4120 accuracy: 0.8890 valid loss 0.4708 accuracy: 0.8810\n",
            "step 15000: train loss 0.3900 accuracy: 0.9033 valid loss 0.4405 accuracy: 0.8930\n",
            "step 15500: train loss 0.3839 accuracy: 0.8907 valid loss 0.4603 accuracy: 0.8817\n",
            "step 16000: train loss 0.3873 accuracy: 0.8977 valid loss 0.4733 accuracy: 0.8850\n",
            "step 16500: train loss 0.3769 accuracy: 0.8990 valid loss 0.4627 accuracy: 0.8957\n",
            "step 17000: train loss 0.4358 accuracy: 0.8863 valid loss 0.5173 accuracy: 0.8827\n",
            "step 17500: train loss 0.3636 accuracy: 0.8930 valid loss 0.4462 accuracy: 0.8840\n",
            "step 18000: train loss 0.3607 accuracy: 0.8990 valid loss 0.4399 accuracy: 0.8907\n",
            "step 18500: train loss 0.3759 accuracy: 0.8897 valid loss 0.4668 accuracy: 0.8843\n",
            "step 19000: train loss 0.3688 accuracy: 0.8987 valid loss 0.4503 accuracy: 0.8887\n",
            "step 19500: train loss 0.3608 accuracy: 0.9047 valid loss 0.4463 accuracy: 0.8840\n",
            "step 20000: train loss 0.3754 accuracy: 0.8930 valid loss 0.4451 accuracy: 0.8793\n",
            "step 20500: train loss 0.4014 accuracy: 0.8873 valid loss 0.4528 accuracy: 0.8780\n",
            "step 21000: train loss 0.3779 accuracy: 0.8980 valid loss 0.4456 accuracy: 0.8860\n",
            "step 21500: train loss 0.3776 accuracy: 0.8953 valid loss 0.4647 accuracy: 0.8843\n",
            "step 22000: train loss 0.3760 accuracy: 0.8880 valid loss 0.4600 accuracy: 0.8810\n",
            "step 22500: train loss 0.4104 accuracy: 0.8787 valid loss 0.4692 accuracy: 0.8747\n",
            "step 23000: train loss 0.3806 accuracy: 0.8877 valid loss 0.4766 accuracy: 0.8890\n",
            "step 23500: train loss 0.3693 accuracy: 0.8950 valid loss 0.4224 accuracy: 0.8863\n",
            "step 24000: train loss 0.3494 accuracy: 0.8983 valid loss 0.4679 accuracy: 0.8867\n",
            "step 24500: train loss 0.3474 accuracy: 0.9030 valid loss 0.4921 accuracy: 0.8907\n",
            "step 25000: train loss 0.3544 accuracy: 0.8980 valid loss 0.4487 accuracy: 0.8793\n",
            "step 25500: train loss 0.3818 accuracy: 0.8983 valid loss 0.4408 accuracy: 0.8903\n",
            "step 26000: train loss 0.3595 accuracy: 0.8940 valid loss 0.4025 accuracy: 0.8947\n",
            "step 26500: train loss 0.3581 accuracy: 0.9023 valid loss 0.4423 accuracy: 0.8877\n",
            "step 27000: train loss 0.3501 accuracy: 0.8983 valid loss 0.4849 accuracy: 0.8843\n",
            "step 27500: train loss 0.3625 accuracy: 0.8977 valid loss 0.4596 accuracy: 0.8897\n",
            "step 28000: train loss 0.3406 accuracy: 0.9020 valid loss 0.3904 accuracy: 0.9043\n",
            "step 28500: train loss 0.3510 accuracy: 0.9027 valid loss 0.4370 accuracy: 0.9030\n",
            "step 29000: train loss 0.3249 accuracy: 0.9073 valid loss 0.4711 accuracy: 0.8790\n",
            "step 29500: train loss 0.3458 accuracy: 0.9030 valid loss 0.4370 accuracy: 0.8963\n",
            "step 29999: train loss 0.3569 accuracy: 0.8947 valid loss 0.4236 accuracy: 0.8973\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h0cZNSReYvt2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}